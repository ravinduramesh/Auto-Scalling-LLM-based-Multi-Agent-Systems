import json
import matplotlib.pyplot as plt
from common_resources_for_evaluators import jsonFilePaths, clean_text, ground_truth_vocab

conversationScores = dict()
ground_truth_vocab = set(ground_truth_vocab)

# Read data from the JSON file
for jsonFilePath in jsonFilePaths:
    with open(jsonFilePath, "r") as jsonFile:
        jsonData = json.load(jsonFile)

    initialAgents = {"Patient", "General-Ward-Doctor", "Nurse"}
    initialAgentsContent = ""
    autoGeneratedAgentsContent = ""

    for entry in jsonData:
        if entry["role"] in initialAgents:
            initialAgentsContent += entry["content"] + " "
            initialAgents.remove(entry["role"])
        else:
            autoGeneratedAgentsContent += entry["content"] + " "

    # Clean the text data and remove stopwords
    initialAgentsCleanedContent = clean_text(initialAgentsContent)
    autoGeneratedAgentsCleanedContent = clean_text(autoGeneratedAgentsContent)

    initialAgentsTermSet = set(initialAgentsCleanedContent.split())
    autoGeneratedAgentsTermSet = set(autoGeneratedAgentsCleanedContent.split())
    termsToScoreConversation = ground_truth_vocab.intersection(autoGeneratedAgentsTermSet - initialAgentsTermSet)

    filename = jsonFilePath.split("/")[-2] + '\n' + jsonFilePath.split("/")[-1]
    conversationScores[filename] = len(termsToScoreConversation)


sorted_items = sorted(conversationScores.items(), key=lambda x: x[1], reverse=True)
sorted_labels = [item[0] for item in sorted_items]
sorted_scores = [item[1] for item in sorted_items]

# Assign colors based on label type for legend
barColors = []
color_map = {"autogen": "orangered", "DRTAG": "lawngreen", "other": "dodgerblue"}
for label in sorted_labels:
    if label.split("\n")[1].startswith("autogen"):
        barColors.append(color_map["autogen"])
    elif label.split("\n")[1].startswith("DRTAG"):
        barColors.append(color_map["DRTAG"])
    else:
        barColors.append(color_map["other"])

plt.figure(figsize=(25, 10))
plt.rcParams.update({'font.size': 15})

bars = plt.bar(sorted_labels, sorted_scores, color=barColors)
plt.xticks(rotation=90)
plt.ylabel("Conversation Score")
plt.title("Conversation scores calculated by binary weighting keywords in new dialogues.")
plt.tight_layout()

# Add legend
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor=color_map["autogen"], label='autogen'),
    Patch(facecolor=color_map["DRTAG"], label='DRTAG'),
    Patch(facecolor=color_map["other"], label='other')
]
plt.legend(handles=legend_elements, loc='upper right', title="Label Type")

plt.savefig("binaryWeightingScoresOfConversations.png")
print("Graph is plotted successfully.")

# Statistical analysis with Mann-Whitney U rank test on binary weighting scores
import numpy as np
from scipy.stats import mannwhitneyu
import scikit_posthocs as sp

standardSignificanceLevel = 0.05
conclusions = []

# Group binary weighting scores by label
autogen_scores = [score for label, score in conversationScores.items() if "autogen" in label]
drtag_scores = [score for label, score in conversationScores.items() if "DRTAG" in label]
iaag_scores = [score for label, score in conversationScores.items() if "IAAG" in label]

autogen_llm_selection_scores = [score for label, score in conversationScores.items() if "autogen-llm-selection" in label]
drtag_llm_selection_scores = [score for label, score in conversationScores.items() if "DRTAG-llm-selection" in label]
iaag_llm_selection_scores = [score for label, score in conversationScores.items() if "IAAG-llm-selection" in label]
autogen_random_selection_scores = [score for label, score in conversationScores.items() if "autogen-random-selection" in label]
drtag_random_selection_scores = [score for label, score in conversationScores.items() if "DRTAG-random-selection" in label]
iaag_random_selection_scores = [score for label, score in conversationScores.items() if "IAAG-random-selection" in label]
autogen_round_robin_selection_scores = [score for label, score in conversationScores.items() if "autogen-round-robin" in label]
drtag_round_robin_selection_scores = [score for label, score in conversationScores.items() if "DRTAG-round-robin" in label]
iaag_round_robin_selection_scores = [score for label, score in conversationScores.items() if "IAAG-round-robin" in label]

# Mann-Whitney U rank test to check if DRTAG is better than Autogen
stat, p = mannwhitneyu(drtag_scores, autogen_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (DRTAG's Binary Weighting scores are better than Autogen's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using DRTAG contains more keywords relavant to the scenario than discussions generated using Autogen.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using DRTAG contains more keywords relavant to the scenario than discussions generated using Autogen.")
conclusions.append("")

# Mann-Whitney U rank test to check if IAAG is better than Autogen
stat, p = mannwhitneyu(iaag_scores, autogen_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (IAAG's Binary Weighting scores are better than Autogen's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using IAAG contains more keywords relavant to the scenario than discussions generated using Autogen.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using IAAG contains more keywords relavant to the scenario than discussions generated using Autogen.")
conclusions.append("")

# Mann-Whitney U rank test to check if DRTAG is better than IAAG
stat, p = mannwhitneyu(drtag_scores, iaag_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (DRTAG's Binary Weighting scores are better than IAAG's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using DRTAG contains more keywords relavant to the scenario than discussions generated using IAAG.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using DRTAG contains more keywords relavant to the scenario than discussions generated using IAAG.")
conclusions.append("")

# Mann-Whitney U rank test to check if DRTAG LLM selection is better than Autogen LLM selection
stat, p = mannwhitneyu(drtag_llm_selection_scores, autogen_llm_selection_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (DRTAG LLM Selection's Binary Weighting scores are better than Autogen LLM Selection's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using DRTAG LLM Selection contains more keywords relavant to the scenario than discussions generated using Autogen LLM Selection.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using DRTAG LLM Selection contains more keywords relavant to the scenario than discussions generated using Autogen LLM Selection.")
conclusions.append("")

# Mann-Whitney U rank test to check if IAAG LLM selection is better than Autogen LLM selection
stat, p = mannwhitneyu(iaag_llm_selection_scores, autogen_llm_selection_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (IAAG LLM Selection's Binary Weighting scores are better than Autogen LLM Selection's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using IAAG LLM Selection contains more keywords relavant to the scenario than discussions generated using Autogen LLM Selection.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using IAAG LLM Selection contains more keywords relavant to the scenario than discussions generated using Autogen LLM Selection.")
conclusions.append("")

# Mann-Whitney U rank test to check if DRTAG Random selection is better than Autogen Random selection
stat, p = mannwhitneyu(drtag_random_selection_scores, autogen_random_selection_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (DRTAG Random Selection's Binary Weighting scores are better than Autogen Random Selection's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using DRTAG Random Selection contains more keywords relavant to the scenario than discussions generated using Autogen Random Selection.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using DRTAG Random Selection contains more keywords relavant to the scenario than discussions generated using Autogen Random Selection.")
conclusions.append("")

# Mann-Whitney U rank test to check if IAAG Random selection is better than Autogen Random selection
stat, p = mannwhitneyu(iaag_random_selection_scores, autogen_random_selection_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (IAAG Random Selection's Binary Weighting scores are better than Autogen Random Selection's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using IAAG Random Selection contains more keywords relavant to the scenario than discussions generated using Autogen Random Selection.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using IAAG Random Selection contains more keywords relavant to the scenario than discussions generated using Autogen Random Selection.")
conclusions.append("")

# Mann-Whitney U rank test to check if DRTAG Round Robin selection is better than Autogen Round Robin selection
stat, p = mannwhitneyu(drtag_round_robin_selection_scores, autogen_round_robin_selection_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (DRTAG Round Robin Selection's Binary Weighting scores are better than Autogen Round Robin Selection's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using DRTAG Round Robin Selection contains more keywords relavant to the scenario than discussions generated using Autogen Round Robin Selection.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using DRTAG Round Robin Selection contains more keywords relavant to the scenario than discussions generated using Autogen Round Robin Selection.")
conclusions.append("")

# Mann-Whitney U rank test to check if IAAG Round Robin selection is better than Autogen Round Robin selection
stat, p = mannwhitneyu(iaag_round_robin_selection_scores, autogen_round_robin_selection_scores, alternative='greater')
conclusions.append(f"Mann-Whitney U Test (IAAG Round Robin Selection's Binary Weighting scores are better than Autogen Round Robin Selection's Binary Weighting scores): H={stat:.3f}, p={p:.4f}")
if p < standardSignificanceLevel:
    conclusions.append("Conclusion: We reject the null hypothesis. There is statistically significant evidence to conclude that discussions generated using IAAG Round Robin Selection contains more keywords relavant to the scenario than discussions generated using Autogen Round Robin Selection.")
else:
    conclusions.append("Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence to conclude that discussions generated using IAAG Round Robin Selection contains more keywords relavant to the scenario than discussions generated using Autogen Round Robin Selection.")
conclusions.append("")

# Save conclusions to a text file
with open("binary-weighting-results-analysis.txt", "w") as file:
    for conclusion in conclusions:
        file.write(conclusion + "\n")
print("All conclusions are written to the file 'binary-weighting-results-analysis-conclusions.txt'.")
